{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from discovery_utils.getters import gtr\n",
    "from discovery_utils.getters import crunchbase\n",
    "from discovery_utils.utils import (\n",
    "    search,\n",
    "    analysis_crunchbase,\n",
    "    analysis,\n",
    "    charts,\n",
    "    viz_landscape,\n",
    ")\n",
    "from discovery_utils.utils.llm.batch_check import LLMProcessor, generate_system_message\n",
    "\n",
    "## change to markup until poetry issue resolved:\n",
    "## from src import PROJECT_DIR\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Paths and directories\n",
    "PROJECT_DIR = Path(\"/Users/william.woodward/Documents/discovery_mission_radar_prototyping\")\n",
    "from src import VECTOR_DB_DIR\n",
    "OUTPUT_DIR = PROJECT_DIR / \"data/2025_01_MS_ahl/\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Crunchbase setup\n",
    "CB = crunchbase.CrunchbaseGetter(vector_db_path=VECTOR_DB_DIR)\n",
    "\n",
    "# List of configuration files\n",
    "# List of configuration files\n",
    "config_files = [\n",
    "    \"config_MS_challenger_brands.yaml\",\n",
    "    \"config_MS_child_obesity.yaml\",\n",
    "    \"config_MS_food_as_medicine.yaml\",\n",
    "    \"config_MS_future_of_ag.yaml\",\n",
    "    \"config_MS_glp1.yaml\",\n",
    "    \"config_MS_hormonal_dysregulation.yaml\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all config files\n",
    "for config_file in config_files:\n",
    "    config_suffix = \"_\".join(config_file.split('_')[2:]).split('.')[0]\n",
    "    print(f\"Processing config: {config_suffix}\")\n",
    "\n",
    "    SearchCB = search.SearchDataset(CB, CB.organisations_enriched, config_file)\n",
    "    search_cb_df = SearchCB.do_search()\n",
    "\n",
    "    search_cb_df[['id', 'name', 'short_description', 'homepage_url', '_score_keywords', '_score_vectors', '_score_avg']]\n",
    "\n",
    "    # Filter for number of companies that are above arbitrary relevance score and check number\n",
    "    relevant_df = search_cb_df.query(\"_score_avg > 0.3\")\n",
    "    print(f\"Number of relevant organisations: {len(relevant_df)}\")\n",
    "\n",
    "    # Add LLM processor to check lists \n",
    "    system_message = generate_system_message(config_file)\n",
    "    fields = [\n",
    "        {\"name\": \"is_relevant\", \"type\": \"str\", \"description\": \"A one-word answer: 'yes' or 'no'.\"},\n",
    "    ]\n",
    "\n",
    "    check_data = dict(zip(relevant_df['id'], relevant_df['short_description']))\n",
    "\n",
    "    processor = LLMProcessor(\n",
    "        output_path=str(OUTPUT_DIR / f\"llm_check_MS_{config_suffix}.jsonl\"),\n",
    "        system_message=system_message,\n",
    "        session_name=\"mission_studio\",\n",
    "        output_fields=fields,\n",
    "    )\n",
    "\n",
    "    processor.run(check_data, batch_size=15, sleep_time=0.5)\n",
    "\n",
    "    str(OUTPUT_DIR / f\"output_{config_suffix}.jsonl\")\n",
    "\n",
    "    file_path = OUTPUT_DIR / f\"llm_check_MS_{config_suffix}.jsonl\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"File successfully saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    relevant_check_df = pd.read_json(file_path, lines=True)\n",
    "    relevant_checked_df = relevant_df.merge(relevant_check_df[['id', 'is_relevant']], left_on='id', right_on='id', how='left')\n",
    "    relevant_checked_df.query(\"is_relevant == 'yes'\")[['id', 'name', 'short_description', 'homepage_url', '_score_avg']]\n",
    "\n",
    "    matching_ids = set(relevant_checked_df.id)\n",
    "    print(f\"Matching IDs: {matching_ids}\")\n",
    "    print(f\"Number of matching IDs: {len(matching_ids)}\")\n",
    "\n",
    "    matchings_orgs_df = CB.organisations_enriched.query(\"id in @matching_ids\")\n",
    "    funding_rounds_df = (\n",
    "        CB.select_funding_rounds(org_ids=matching_ids, funding_round_types=[\"angel\", \"pre_seed\", \"seed\", \"series_a\", \"series_b\"])\n",
    "    )\n",
    "\n",
    "    # Organise investors by each funding round\n",
    "    investors_df = (\n",
    "        CB.funding_rounds_enriched\n",
    "        .query(\"funding_round_id in @funding_rounds_df.funding_round_id\")\n",
    "        .groupby(\"funding_round_id\")\n",
    "        .agg(investor_name=(\"investor_name\", list))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    funding_rounds_df = (\n",
    "        funding_rounds_df\n",
    "        .drop(columns=[\"investor_name\"])\n",
    "        .merge(investors_df, on=\"funding_round_id\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    funding_rounds_df.to_csv(f\"{OUTPUT_DIR}/{config_suffix}_funding_rounds.csv\", index=False)\n",
    "\n",
    "    # organise investors by each funding round\n",
    "    len(funding_rounds_df)\n",
    "\n",
    "    # save funding rounds as csv\n",
    "    funding_rounds_df.to_csv(f\"{OUTPUT_DIR}/{config_suffix}_funding_rounds.csv\", index=False)\n",
    "\n",
    "    # Generate time series\n",
    "    ts_df = analysis_crunchbase.get_timeseries(matchings_orgs_df, funding_rounds_df, period='year', min_year=2014, max_year=2025)\n",
    "    \n",
    "    # Breakdown of deal types\n",
    "    deals_df, deal_counts_df = analysis_crunchbase.get_funding_by_year_and_range(funding_rounds_df, 2014, 2025)\n",
    "    aggregated_funding_types_df = analysis_crunchbase.aggregate_by_funding_round_types(funding_rounds_df)\n",
    "\n",
    "    # Chart by deal counts and save\n",
    "    investment_types_counts_fig = analysis_crunchbase.chart_investment_types_counts(aggregated_funding_types_df)\n",
    "    investment_types_counts_chart_filename = f\"{OUTPUT_DIR}/charts/{config_suffix}_investment_types_counts.png\"\n",
    "    investment_types_counts_fig.save(investment_types_counts_chart_filename)\n",
    "\n",
    "    # Chart by deal types (deal size amounts)\n",
    "    investment_types_fig = analysis_crunchbase.chart_investment_types(aggregated_funding_types_df)\n",
    "    investment_types_chart_filename = f\"{OUTPUT_DIR}/charts/{config_suffix}_investment_types.png\"\n",
    "    investment_types_fig.save(investment_types_chart_filename)\n",
    "\n",
    "    # Chart companies founded\n",
    "    fig = charts.ts_bar(\n",
    "        ts=ts_df,\n",
    "        variable=\"n_orgs_founded\",\n",
    "        variable_title=\"Number of companies founded\"\n",
    "    )\n",
    "    companies_founded_chart_filename = f\"{OUTPUT_DIR}/charts/{config_suffix}_companies_founded.png\"\n",
    "    fig.save(companies_founded_chart_filename)\n",
    "\n",
    "    # Generate market map\n",
    "    id_condition = \"id in ('{}')\".format(\"', '\".join(list(matching_ids)))\n",
    "    vectors_df = CB.VectorDB.vector_db.search().where(id_condition).limit(30000).to_pandas()\n",
    "\n",
    "    fig, cb_viz_df = viz_landscape.generate_crunchbase_landscape(vectors_df, CB, min_cluster_size=15)\n",
    "    output_path = f\"{OUTPUT_DIR}/charts/{config_suffix}_market_map.html\"\n",
    "    fig.save(str(output_path))\n",
    "\n",
    "    # Filter companies with exits greater than zero\n",
    "    companies_with_exits = matchings_orgs_df[matchings_orgs_df['num_exits'] > 0]\n",
    "    print(companies_with_exits[['name', 'num_exits']])\n",
    "\n",
    "    companies_with_exits[['name', 'num_exits']].to_csv(f\"{OUTPUT_DIR}/{config_suffix}_exits.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of new companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.utils.analysis_crunchbase import get_timeseries\n",
    "from discovery_utils.utils import charts\n",
    "from discovery_utils.utils import analysis\n",
    "\n",
    "matching_ids = relevant_checked_df.query(\"is_relevant == 'yes'\").id.to_list()\n",
    "\n",
    "ts_df = get_timeseries(\n",
    "    cb_orgs = CB.organisations_enriched.query(\"id in @matching_ids\"),\n",
    "    cb_funding_rounds = CB.funding_rounds_enriched.query(\"org_id in @matching_ids\"),\n",
    "    min_year = 2010,\n",
    "    max_year = 2025,\n",
    "    period='year',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charts.ts_bar(\n",
    "    ts=ts_df,\n",
    "    variable = \"n_orgs_founded\",\n",
    "    variable_title = \"Number of organisations founded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.smoothed_growth(ts_df, year_start=2020, year_end=2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â remove duplicates\n",
    "CB.funding_rounds_enriched.drop_duplicates(subset=[\"funding_round_id\"]).duplicated(\"funding_round_id\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = get_timeseries(\n",
    "    cb_orgs = CB.organisations_enriched,\n",
    "    cb_funding_rounds = CB.funding_rounds_enriched.query(\"org_id in @matching_ids\"),\n",
    "    min_year = 2010,\n",
    "    max_year = 2025,\n",
    "    period='year',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.utils import viz_landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_condition = \"id in ('{}')\".format(\"', '\".join(list(matching_ids)))\n",
    "vectors_df = CB.VectorDB.vector_db.search().where(id_condition).limit(30000).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, cb_viz_df = viz_landscape.generate_crunchbase_landscape(vectors_df, CB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"test.html\"\n",
    "fig.save(str(output_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery-mission-radar-prototyping-uqNXz-mU-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
