{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of using analysis functionalities\n",
    "\n",
    "Using discovery_utils analyses functionalities for investments data\n",
    "\n",
    "Here, we'll find companies using their categories, but you can also use search results from the process shown in cybersec_search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.utils import (\n",
    "    analysis_crunchbase,\n",
    "    analysis,\n",
    "    charts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.getters import crunchbase\n",
    "CB = crunchbase.CrunchbaseGetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CB.vector_db_path = \"tmp/vector_db\"\n",
    "CB.VectorDB = crunchbase.embeddings.VectorDB(\n",
    "    db_path=PROJECT_DIR / \"tmp/vector_db\",\n",
    "    db_name=\"crunchbase-lancedb\",\n",
    "    table_name=\"company_embeddings\",\n",
    "    model=\"all-MiniLM-L6-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting companies using our categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_weight = CB.organisations_enriched.topic_labels.str.contains('Weight').astype(bool)\n",
    "orgs_weight_df = CB.organisations_enriched[has_weight]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {\n",
    "    \"Publishing\": \"Yes\",\n",
    "}\n",
    "\n",
    "(\n",
    "    CB.organisation_categories\n",
    "    .head(10)\n",
    "    .explode(\"category_list\")\n",
    "    .category_list\n",
    "    .apply(lambda x: map_dict.get(x, \"Nan\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting companies using CB categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories for cybersec\n",
    "CB.find_similar_categories(\"obesity, diabetes, nutrition\", category_type=\"narrow\", n_results=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists_of_categories = {\n",
    "#     \"health_diabetes\": [\"Diabetes\"],\n",
    "#     \"health_nutrition\": [\"Nutrition\", \"Dietary supplements\"],\n",
    "#     \"biology\": [\"AgTech\", \"Agriculture\"],\n",
    "#     \"economic\": [\"Farmers Market\", \"Food Delivery\", \"Food Processing\", \"Food Trucks\", \"Food and Beverage\", \"Grocery\", \"Organic Food\", \"Restaurants\", \"Snack Food\", ],\n",
    "#     \"social\": [\"Recipes\"],\n",
    "# }\n",
    "\n",
    "lists_of_categories = {\n",
    "    \"biological\": [\"Agtech\", \"Agriculture\"],\n",
    "    \"health_diabetes\": [\"Diabetes\"],\n",
    "    \"health_nutrition\": [\"Dietary Supplements\", \"Nutrition\"],\n",
    "    \"economic_retail\": [\"Organic Food\", \"Grocery\", \"Snack Food\", \"Farmers Market\"],\n",
    "    \"economic_ooh\": [\"Food Delivery\", \"Restaurants\"],\n",
    "    \"economic_food_proc\": [\"Food Processing\"],\n",
    "    \"food_beverage\": [\"Food and Beverage\"],\n",
    "    \"social\": [\"Recipes\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = \"social\"\n",
    "list_of_categories = lists_of_categories[save_name]\n",
    "print(list_of_categories)\n",
    "selected_df = CB.get_companies_in_categories(list_of_categories, category_type=\"narrow\")\n",
    "\n",
    "matching_ids = set(list(selected_df.id.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_recent_companies = False\n",
    "if only_recent_companies:\n",
    "    ids_recent = CB.organisations_enriched.query(\"last_funding_on > '2019'\").id.to_list()\n",
    "    ids_new = CB.organisations_enriched.query(\"founded_on > '2019'\").id.to_list()\n",
    "    recent_or_new = set(ids_recent + ids_new)\n",
    "    matching_ids = matching_ids.intersection(recent_or_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matching_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write an sql query to achieve id in test_ids\n",
    "id_condition = \"id in ('{}')\".format(\"', '\".join(list(matching_ids)))\n",
    "vectors_df = CB.VectorDB.vector_db.search().where(id_condition).limit(30000).to_pandas()\n",
    "len(vectors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertopic\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bertopic.representation import OpenAI\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "# Create your representation model\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "representation_model = OpenAI(client, model=\"gpt-4o-mini\", delay_in_seconds=1, chat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_CLUSTER_SIZE = 15\n",
    "MIN_CLUSTER_SIZE = 50\n",
    "\n",
    "# Initialize BERTopic\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "custom_hdbscan = HDBSCAN(\n",
    "    min_cluster_size=MIN_CLUSTER_SIZE,  # Minimum size for clusters\n",
    "    prediction_data=True,  # Allow prediction for new data points\n",
    "    cluster_selection_method='leaf',\n",
    "    metric='euclidean',\n",
    ")\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15,\n",
    "            n_components=5,\n",
    "            min_dist=0.0,\n",
    "            metric='euclidean',\n",
    "            low_memory=False,\n",
    "            random_state=42)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    min_topic_size=MIN_CLUSTER_SIZE,  # Set the minimum size for a cluster\n",
    "    n_gram_range=(1, 1),  # Set the n-gram range for topic extraction\n",
    "    verbose=True,  # Enable verbose output for progress tracking\n",
    "    hdbscan_model=custom_hdbscan,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    umap_model=umap_model,\n",
    "    representation_model=representation_model,\n",
    "    nr_topics=10,\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model using precomputed embeddings\n",
    "topics, probs = topic_model.fit_transform(\n",
    "    vectors_df['text'], \n",
    "    embeddings=np.array(vectors_df['vector'].to_list()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df['topics'] = topics\n",
    "vectors_df['topics'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    new_topics = topic_model.reduce_outliers(\n",
    "        vectors_df['text'].to_list(),\n",
    "        topics=vectors_df['topics'].to_list(),\n",
    "        strategy='embeddings',\n",
    "        embeddings=np.array(vectors_df['vector'].to_list())\n",
    "    )\n",
    "    vectors_df['new_topic'] = new_topics\n",
    "    print(\"Outliers reduced\")\n",
    "except Exception as e:\n",
    "    vectors_df['new_topic'] = topics\n",
    "vectors_df = vectors_df.assign(new_topic = lambda df: df.new_topic.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import altair as alt\n",
    "# max rows\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_embeddings = (\n",
    "    UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine')\n",
    "    .fit_transform(np.array(vectors_df['vector'].to_list()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = (\n",
    "    vectors_df\n",
    "    .assign(\n",
    "        umap_x = reduced_embeddings[:, 0],\n",
    "        umap_y = reduced_embeddings[:, 1],\n",
    "    )\n",
    "    .merge(\n",
    "        topic_model.get_topic_info(), left_on='new_topic', right_on='Topic', how='left'\n",
    "    )\n",
    "    # .merge(\n",
    "    #     topic_names, left_on='Topic', right_on='Cluster', how='left'\n",
    "    # )\n",
    "    # .drop(columns=['charity_activities_vector', 'charity_activities']) \n",
    "    # .merge(\n",
    "    #     data_df[['registered_charity_number','charity_activities']]\n",
    "    #     .astype({'registered_charity_number': str}), on='registered_charity_number', how='left'\n",
    "    # )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(viz_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import Iterator, Dict\n",
    "from collections import defaultdict\n",
    "\n",
    "simple_tokenizer = lambda x: x.split()\n",
    "\n",
    "\n",
    "def cluster_texts(documents: Iterator[str], cluster_labels: Iterator) -> Dict:\n",
    "    \"\"\"\n",
    "    Creates a large text string for each cluster, by joining up the\n",
    "    text strings (documents) belonging to the same cluster\n",
    "    Args:\n",
    "        documents: A list of text strings\n",
    "        cluster_labels: A list of cluster labels, indicating the membership of the text strings\n",
    "    Returns:\n",
    "        A dictionary where keys are cluster labels, and values are cluster text documents\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(documents) == len(cluster_labels)\n",
    "    doc_type = type(documents[0])\n",
    "\n",
    "    cluster_text_dict = defaultdict(doc_type)\n",
    "    for i, doc in enumerate(documents):\n",
    "        if doc_type is str:\n",
    "            cluster_text_dict[cluster_labels[i]] += doc + \" \"\n",
    "        elif doc_type is list:\n",
    "            cluster_text_dict[cluster_labels[i]] += doc\n",
    "    return cluster_text_dict\n",
    "\n",
    "\n",
    "def cluster_keywords(\n",
    "    documents: Iterator[str],\n",
    "    cluster_labels: Iterator[int],\n",
    "    n: int = 10,\n",
    "    tokenizer=simple_tokenizer,\n",
    "    max_df: float = 0.90,\n",
    "    min_df: float = 0.01,\n",
    "    Vectorizer=TfidfVectorizer,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Generates keywords that characterise the cluster, using the specified Vectorizer\n",
    "    Args:\n",
    "        documents: List of (preprocessed) text documents\n",
    "        cluster_labels: List of integer cluster labels\n",
    "        n: Number of top keywords to return\n",
    "        Vectorizer: Vectorizer object to use (eg, TfidfVectorizer, CountVectorizer)\n",
    "        tokenizer: Function to use to tokenise the input documents; by default splits the document into words\n",
    "    Returns:\n",
    "        Dictionary that maps cluster integer labels to a list of keywords\n",
    "    \"\"\"\n",
    "\n",
    "    # Define vectorizer\n",
    "    vectorizer = Vectorizer(\n",
    "        analyzer=\"word\",\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=lambda x: x,\n",
    "        token_pattern=None,\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=10000,\n",
    "    )\n",
    "\n",
    "    # Create cluster text documents\n",
    "    cluster_documents = cluster_texts(documents, cluster_labels)\n",
    "    unique_cluster_labels = list(cluster_documents.keys())\n",
    "\n",
    "    # Apply the vectorizer\n",
    "    token_score_matrix = vectorizer.fit_transform(list(cluster_documents.values()))\n",
    "\n",
    "    # Create a token lookup dictionary\n",
    "    id_to_token = dict(\n",
    "        zip(list(vectorizer.vocabulary_.values()), list(vectorizer.vocabulary_.keys()))\n",
    "    )\n",
    "\n",
    "    # For each cluster, check the top n tokens\n",
    "    top_cluster_tokens = {}\n",
    "    for i in range(token_score_matrix.shape[0]):\n",
    "        # Get the cluster feature vector\n",
    "        x = token_score_matrix[i, :].todense()\n",
    "        # Find the indices of the top n tokens\n",
    "        x = list(np.flip(np.argsort(np.array(x)))[0])[0:n]\n",
    "        # Find the tokens corresponding to the top n indices\n",
    "        top_cluster_tokens[unique_cluster_labels[i]] = [id_to_token[j] for j in x]\n",
    "\n",
    "    return top_cluster_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "full_stopwords = stopwords.words(\"english\")# + [\"family\", \"aged\", \"object\"]\n",
    "def preproc(text: str) -> str:\n",
    "    text = re.sub(r\"[^a-zA-Z ]+\", \"\", text).lower()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(t) for t in text]\n",
    "    text = [t for t in text if t not in full_stopwords]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = KMeans(n_clusters=35, random_state=10)\n",
    "clusterer.fit(viz_df[[\"umap_x\", \"umap_y\"]])\n",
    "soft_clusters = list(clusterer.labels_)\n",
    "soft_cluster = [np.argmax(x) for x in soft_clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_texts = viz_df[\"text\"].apply(preproc)\n",
    "_cluster_texts = cluster_texts(title_texts, soft_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_cluster_keywords = cluster_keywords(\n",
    "    documents=list(_cluster_texts.values()),\n",
    "    cluster_labels=list(_cluster_texts.keys()),\n",
    "    n=2,\n",
    "    max_df=0.90,\n",
    "    min_df=0.01,\n",
    "    Vectorizer=TfidfVectorizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df[\"soft_cluster\"] = soft_clusters\n",
    "viz_df[\"soft_cluster_\"] = [str(x) for x in soft_clusters]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = (\n",
    "    viz_df.groupby(\"soft_cluster\")\n",
    "    .agg(x_c=(\"umap_x\", \"mean\"), y_c=(\"umap_y\", \"mean\"))\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        keywords=lambda x: x.soft_cluster.apply(\n",
    "            lambda y: \", \".join(_cluster_keywords[y])\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "# remove max rows\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    alt.Chart(centroids)\n",
    "    .mark_text(\n",
    "        fontSize=13.5,\n",
    "        fontStyle=\"bold\",\n",
    "        opacity=0.8,\n",
    "        stroke=\"white\",\n",
    "        strokeWidth=1,\n",
    "        strokeOffset=0,\n",
    "        strokeOpacity=0.4,\n",
    "    )\n",
    "    .encode(x=alt.X(\"x_c:Q\"), y=alt.Y(\"y_c:Q\"), text=alt.Text(\"keywords\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs_df = CB.organisations_enriched.query(\"id in @viz_df.id.to_list()\")[[\"id\", \"last_funding_on\", \"total_funding_gbp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mapping = {\n",
    "    # North America + Australia\n",
    "    'USA': 'North America + Australia',\n",
    "    'CAN': 'North America + Australia',\n",
    "    'AUS': 'North America + Australia',\n",
    "    'NZL': 'North America + Australia',\n",
    "\n",
    "    # South America (Including Mexico and Central America)\n",
    "    'VEN': 'South + Central America',\n",
    "    'ARG': 'South + Central America',\n",
    "    'BRA': 'South + Central America',\n",
    "    'CHL': 'South + Central America',\n",
    "    'COL': 'South + Central America',\n",
    "    'PER': 'South + Central America',\n",
    "    'URY': 'South + Central America',\n",
    "    'PRY': 'South + Central America',\n",
    "    'ECU': 'South + Central America',\n",
    "    'BOL': 'South + Central America',\n",
    "    'GUY': 'South + Central America',\n",
    "    'SUR': 'South + Central America',\n",
    "    'MEX': 'South + Central America',\n",
    "    'CRI': 'South + Central America',\n",
    "    'SLV': 'South + Central America',\n",
    "    'GTM': 'South + Central America',\n",
    "    'HND': 'South + Central America',\n",
    "    'PAN': 'South + Central America',\n",
    "    'NIC': 'South + Central America',\n",
    "\n",
    "    # Europe\n",
    "    'IRL': 'Europe',\n",
    "    'LUX': 'Europe',\n",
    "    'CHE': 'Europe',\n",
    "    'ESP': 'Europe',\n",
    "    'DEU': 'Europe',\n",
    "    'FRA': 'Europe',\n",
    "    'FIN': 'Europe',\n",
    "    'SWE': 'Europe',\n",
    "    'NLD': 'Europe',\n",
    "    'BEL': 'Europe',\n",
    "    'DNK': 'Europe',\n",
    "    'CZE': 'Europe',\n",
    "    'POL': 'Europe',\n",
    "    'EST': 'Europe',\n",
    "    'AUT': 'Europe',\n",
    "    'ITA': 'Europe',\n",
    "    'ROU': 'Europe',\n",
    "    'CYP': 'Europe',\n",
    "    'NOR': 'Europe',\n",
    "    'PRT': 'Europe',\n",
    "    'BGR': 'Europe',\n",
    "    'BLR': 'Europe',\n",
    "    'SVN': 'Europe',\n",
    "    'ARM': 'Europe',\n",
    "    'HUN': 'Europe',\n",
    "    'ISL': 'Europe',\n",
    "    'LVA': 'Europe',\n",
    "    'LTU': 'Europe',\n",
    "    'HRV': 'Europe',\n",
    "    'MKD': 'Europe',\n",
    "    'BIH': 'Europe',\n",
    "    'SRB': 'Europe',\n",
    "    'SVK': 'Europe',\n",
    "    'GEO': 'Europe',\n",
    "    'MDA': 'Europe',\n",
    "    'ALB': 'Europe',\n",
    "    'SMR': 'Europe',\n",
    "    'AND': 'Europe',\n",
    "    'GIB': 'Europe',\n",
    "    'FRO': 'Europe',\n",
    "    'LIE': 'Europe',\n",
    "    'IMN': 'Europe',\n",
    "    'GGY': 'Europe',\n",
    "    'JEY': 'Europe',\n",
    "    'ALA': 'Europe',\n",
    "\n",
    "    # UK\n",
    "    'GBR': 'UK',\n",
    "\n",
    "    # Asia\n",
    "    'IND': 'Asia',\n",
    "    'HKG': 'Asia',\n",
    "    'ISR': 'Asia',\n",
    "    'RUS': 'Asia',\n",
    "    'KOR': 'Asia',\n",
    "    'SGP': 'Asia',\n",
    "    'JPN': 'Asia',\n",
    "    'ARE': 'Asia',\n",
    "    'CHN': 'Asia',\n",
    "    'PHL': 'Asia',\n",
    "    'IDN': 'Asia',\n",
    "    'THA': 'Asia',\n",
    "    'TUR': 'Asia',\n",
    "    'MYS': 'Asia',\n",
    "    'TWN': 'Asia',\n",
    "    'PAK': 'Asia',\n",
    "    'LBN': 'Asia',\n",
    "    'ARM': 'Asia',\n",
    "    'BGD': 'Asia',\n",
    "    'KWT': 'Asia',\n",
    "    'VNM': 'Asia',\n",
    "    'MDV': 'Asia',\n",
    "    'JOR': 'Asia',\n",
    "    'LKA': 'Asia',\n",
    "    'IRN': 'Asia',\n",
    "    'SYR': 'Asia',\n",
    "    'KAZ': 'Asia',\n",
    "    'UZB': 'Asia',\n",
    "    'IRQ': 'Asia',\n",
    "    'OMN': 'Asia',\n",
    "    'PSE': 'Asia',\n",
    "    'TJK': 'Asia',\n",
    "    'BTN': 'Asia',\n",
    "    'TLS': 'Asia',\n",
    "    'MAC': 'Asia',\n",
    "    'MMR': 'Asia',\n",
    "    'MNG': 'Asia',\n",
    "    'KHM': 'Asia',\n",
    "    'LAO': 'Asia',\n",
    "    'BRN': 'Asia',\n",
    "\n",
    "    # Africa\n",
    "    'ZAF': 'Africa',\n",
    "    'MUS': 'Africa',\n",
    "    'EGY': 'Africa',\n",
    "    'GHA': 'Africa',\n",
    "    'KEN': 'Africa',\n",
    "    'NGA': 'Africa',\n",
    "    'MAR': 'Africa',\n",
    "    'CIV': 'Africa',\n",
    "    'ETH': 'Africa',\n",
    "    'TUN': 'Africa',\n",
    "    'MOZ': 'Africa',\n",
    "    'UGA': 'Africa',\n",
    "    'SEN': 'Africa',\n",
    "    'ZWE': 'Africa',\n",
    "    'RWA': 'Africa',\n",
    "    'SDN': 'Africa',\n",
    "    # Add more African countries as needed...\n",
    "\n",
    "    # Middle East\n",
    "    'SAU': 'Middle East',\n",
    "    'ARE': 'Middle East',\n",
    "    'KWT': 'Middle East',\n",
    "    'QAT': 'Middle East',\n",
    "    'OMN': 'Middle East',\n",
    "    'IRQ': 'Middle East',\n",
    "    'IRN': 'Middle East',\n",
    "    'SYR': 'Middle East',\n",
    "    'JOR': 'Middle East',\n",
    "    'LBN': 'Middle East',\n",
    "    'ISR': 'Middle East',\n",
    "    'YEM': 'Middle East',\n",
    "\n",
    "    # Rest of the World\n",
    "    None: 'Rest of the World',\n",
    "    'BMU': 'Rest of the World',\n",
    "    'TTO': 'Rest of the World',\n",
    "    'GLP': 'Rest of the World',\n",
    "    'CYM': 'Rest of the World',\n",
    "    'IMN': 'Rest of the World',\n",
    "    # Add any small or undefined territories here...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_viz_df = (\n",
    "    viz_df.merge(orgs_df, on=\"id\", how=\"left\")\n",
    "    .assign(recent_funding = lambda x: x.last_funding_on > \"2019\")\n",
    "    .astype({\"recent_funding\": str})\n",
    "    .rename(columns={'name': 'title', 'text': 'description', 'Name': 'category'})\n",
    "    .fillna({'total_funding_gbp': 0})\n",
    "    .assign(total_funding_gbp = lambda df: df.total_funding_gbp.apply(lambda x: round(x/1e+3,3)))\n",
    "    .assign(region = lambda x: x.country_code.apply(lambda y: region_mapping.get(y, \"Rest of the World\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_viz_df.recent_funding.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropdown menu for 'recent_funding'\n",
    "recent_funding_dropdown = alt.binding_select(\n",
    "    options=[None] + list(sorted(list(_viz_df['recent_funding'].unique()))),  # Add 'None' for reset\n",
    "    name=\"Recent Funding:\"\n",
    ")\n",
    "recent_funding_selection = alt.selection_point(\n",
    "    fields=['recent_funding'],\n",
    "    bind=recent_funding_dropdown,\n",
    "    name=\"SelectFunding\"\n",
    ")\n",
    "\n",
    "# Dropdown menu for 'Name'\n",
    "name_dropdown = alt.binding_select(\n",
    "    options=[None] + list(sorted(list(_viz_df['category'].unique()))),  # Add 'None' for reset\n",
    "    name=\"Category:\"\n",
    ")\n",
    "name_selection = alt.selection_point(\n",
    "    fields=['category'],\n",
    "    bind=name_dropdown,\n",
    "    name=\"SelectName\"\n",
    ")\n",
    "\n",
    "# Dropdown menu for 'region'\n",
    "region_dropdown = alt.binding_select(\n",
    "    options=[None] + list(sorted(list(_viz_df['region'].unique()))),  # Add 'None' for reset\n",
    "    name=\"Region:\"\n",
    ")\n",
    "region_selection = alt.selection_point(\n",
    "    fields=['region'],\n",
    "    bind=region_dropdown,\n",
    "    name=\"SelectRegion\"\n",
    ")\n",
    "\n",
    "# Scatterplot with dropdown filters\n",
    "fig = (\n",
    "    alt.Chart(_viz_df, width=900, height=750)\n",
    "    .mark_point(size=30, opacity=0.5)\n",
    "    .encode(\n",
    "        x=alt.X(\"umap_x:Q\", axis=None),\n",
    "        y=alt.Y(\"umap_y:Q\", axis=None),\n",
    "        tooltip=[\"title\", \"description\", \"country_code\", \"region\", \"category\", \"homepage_url\", 'last_funding_on', 'total_funding_gbp'],\n",
    "        color=alt.Color(\"category\", legend=alt.Legend(title=\"Category\",  labelLimit=300)),\n",
    "        shape=alt.Shape(\"recent_funding\", legend=alt.Legend(title=\"Recent funding (since 2020)\")),\n",
    "        opacity=alt.condition(\n",
    "            recent_funding_selection & name_selection & region_selection, alt.value(0.5), alt.value(0.0)\n",
    "        ),\n",
    "        href=\"homepage_url\"\n",
    "    )\n",
    "    .add_params(\n",
    "        recent_funding_selection,\n",
    "        name_selection,\n",
    "        region_selection,\n",
    "    )\n",
    "    .interactive()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_final = (\n",
    "    (fig + text)\n",
    "    .configure_axis(\n",
    "        # gridDash=[1, 7],\n",
    "        gridColor=\"white\",\n",
    "        # remove axis all together\n",
    "        domain=False,\n",
    "\n",
    "    )\n",
    "    .configure_view(strokeWidth=0, strokeOpacity=0)\n",
    "    .properties(\n",
    "        # title={\n",
    "        #     \"anchor\": \"start\",\n",
    "        #     \"text\": [\"Children and parenting app landscape\"],\n",
    "        #     \"subtitle\": [\n",
    "        #         \"Each app is visualised as a circle, with similar apps located closer together\",\n",
    "        #     ],\n",
    "        #     \"subtitleFont\": pu.FONT,\n",
    "        #     \"subtitleFontSize\": 14,\n",
    "        # },\n",
    "    )\n",
    "    .interactive()\n",
    ")\n",
    "\n",
    "# fig_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = PROJECT_DIR / f'data/2025_01_MS_ahl/landscape_{save_name}.html'\n",
    "_viz_df.to_csv(PROJECT_DIR / f\"data/2025_01_MS_ahl/table_{save_name}.csv\", index=False)\n",
    "fig_final.save(str(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some basic time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = analysis_crunchbase.get_timeseries(matchings_orgs_df, funding_rounds_df, period='year', min_year=2014, max_year=2024)\n",
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = charts.ts_bar(\n",
    "    ts_df,\n",
    "    variable='raised_amount_gbp_total',\n",
    "    variable_title=\"Raised amount, £ millions\",\n",
    "    category_column=\"_category\",\n",
    ")\n",
    "charts.configure_plots(fig, chart_title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into breakdown of deal types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deals_df, deal_counts_df = analysis_crunchbase.get_funding_by_year_and_range(funding_rounds_df, 2014, 2024)\n",
    "aggregated_funding_types_df = analysis_crunchbase.aggregate_by_funding_round_types(funding_rounds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_funding_types_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deal_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_crunchbase.chart_investment_types(aggregated_funding_types_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_crunchbase.chart_investment_types_counts(aggregated_funding_types_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_crunchbase.chart_deal_sizes(deals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_crunchbase.chart_deal_sizes_counts(deal_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery-mission-radar-prototyping-ejbE0IFh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
