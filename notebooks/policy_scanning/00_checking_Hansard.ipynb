{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.getters import hansard\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:21:01,660 - discovery_utils.getters.hansard - INFO - Downloading debates parquet file: data/policy_scanning_data/enriched/HansardDebates.parquet\n",
      "2024-12-16 18:22:14,760 - discovery_utils.getters.hansard - INFO - Attempting to download label store: data/policy_scanning_data/enriched/HansardDebates_LabelStore_keywords.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2024-12-12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hansard = hansard.HansardGetter()\n",
    "debates_df = Hansard.get_debates_parquet()\n",
    "labelstore_df = Hansard.get_labelstore()\n",
    "debates_df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:23:48,465 - discovery_utils.getters.hansard - INFO - Downloading people metadata\n",
      "2024-12-16 18:23:51,407 - discovery_utils.getters.hansard - INFO - Successfully downloaded and saved people metadata\n"
     ]
    }
   ],
   "source": [
    "people_dict = Hansard.get_people_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from src import synthesis_utils\n",
    "importlib.reload(synthesis_utils);\n",
    "import numpy as np\n",
    "from typing import Literal, Tuple, List, Dict\n",
    "\n",
    "from src import logging\n",
    "from discovery_utils.utils import keywords\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slack_sdk.webhook import WebhookClient\n",
    "import os\n",
    "slack_webhook = WebhookClient(os.environ[\"SLACK_WEBHOOK_URL_TESTING\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def mission_header(mission: str) -> Dict:\n",
    "    \"\"\"Construct mission header block\"\"\"\n",
    "    if mission == \"ASF\":\n",
    "        mission_header = \":potted_plant: *A Sustainable Future*\"\n",
    "    elif mission == \"AFS\":\n",
    "        mission_header = \":hatched_chick: *A Fairer Start*\"\n",
    "    elif mission == \"AHL\":\n",
    "        mission_header = \":mending_heart: *A Healthier Life*\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid mission: {mission}\")\n",
    "\n",
    "    return {\"type\": \"section\", \"text\": {\"type\": \"mrkdwn\", \"text\": mission_header}}\n",
    "\n",
    "def message_header(message_date:str, data_start_date:str, data_end_date:str) -> List[Dict]:\n",
    "    \"\"\"Construct message header block\n",
    "    \n",
    "    Args:\n",
    "        message_date (str): Date when posting the message, in format DD-MM-YYYY\n",
    "        data_start_date (str): Start date of the data, in format DD-MM-YYYY\n",
    "        data_end_date (str): End date of the data, in format DD-MM-YYYY\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of blocks including a header and a context block, where \n",
    "            context block indicates data sources and date range\n",
    "    \"\"\"\n",
    "    header = {\n",
    "        \"type\": \"header\",\n",
    "        \"text\": {\n",
    "            \"type\": \"plain_text\",\n",
    "            \"text\": f\"Policy update {message_date}\",\n",
    "        }\n",
    "    }\n",
    "    context = {\n",
    "        \"type\": \"context\",\n",
    "        \"elements\": [\n",
    "            {\n",
    "                \"type\": \"mrkdwn\",\n",
    "                \"text\": f\"House of Commons debates ({data_start_date} - {data_end_date})\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return [header, context]\n",
    "    \n",
    "\n",
    "def divider() -> Dict:\n",
    "    \"\"\"Construct a divider block\"\"\"\n",
    "    return {\"type\": \"divider\"}\n",
    "\n",
    "def _bullet_point_string(points: List[str]) -> str:\n",
    "    \"\"\"Construct a string of bullet points from a list of strings\"\"\"\n",
    "    return \"\\n\".join([f\"• {point}\" for point in points])     \n",
    "\n",
    "def debate_summary(debate: Dict) -> Dict:\n",
    "    \"\"\"Construct a block for a single debate summary\n",
    "    \n",
    "    Args:\n",
    "        debate (Dict): Dictionary with keys \"title\", \"summary\", \"positives\", \"negatives\", and \"next_steps\".\n",
    "            For example: {\n",
    "                \"title\": \"Title of the debate\",\n",
    "                \"purpose\": \"Summary of the debate\",\n",
    "                \"positives\": [\"Positive point 1\", \"Positive point 2\"],\n",
    "                \"negatives\": [\"Negative point 1\", \"Negative point 2\"],\n",
    "                \"next_steps\": [\"Next step 1\", \"Next step 2\"],\n",
    "            }\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"type\": \"section\",\n",
    "        \"text\": {\n",
    "            \"type\": \"mrkdwn\",\n",
    "            \"text\": f\"<{debate['url']}|*{debate['title']}*> ({debate['date']})\\n{debate['purpose']}\"\n",
    "        }\n",
    "    }\n",
    "    positives = {\n",
    "        \"type\": \"section\",\n",
    "        \"text\":{\n",
    "             \"type\": \"mrkdwn\",\n",
    "             \"text\": f\"*Positives*\\n{_bullet_point_string(debate['positives'])}\"\n",
    "        }\n",
    "    }\n",
    "    negatives = {\n",
    "        \"type\": \"section\",\n",
    "        \"text\":{\n",
    "             \"type\": \"mrkdwn\",\n",
    "             \"text\": f\"*Negatives*\\n{_bullet_point_string(debate['negatives'])}\"\n",
    "        }\n",
    "    }\n",
    "    next_steps = {\n",
    "        \"type\": \"section\",\n",
    "        \"text\":{\n",
    "             \"type\": \"mrkdwn\",\n",
    "             \"text\": f\"*Next Steps*\\n{_bullet_point_string(debate['next_steps'])}\"\n",
    "        }\n",
    "    }\n",
    "    return [summary, positives, negatives, next_steps, divider()]\n",
    "\n",
    "def quote_block(quote: Dict) -> Dict:\n",
    "    \"\"\"Construct a block with a quote\n",
    "    \n",
    "    Args:\n",
    "        quote (Dict): Dictionary with keys \"name\", \"party\", \"category\", \"debate\", and \"text\".\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"type\": \"section\",\n",
    "        \"text\": {\n",
    "            \"type\": \"mrkdwn\",\n",
    "            \"text\": f\"*{quote['name']}* ({quote['party']}) mentioned *{quote['category']}* in *{quote['debate']}*\\n\\n> {quote['text']}\"\n",
    "        }\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def people_party_memberships(people_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"Get the most recent party membership for each person\n",
    "    \n",
    "    Args:\n",
    "        people_dict: The dictionary of people metadata from the Hansard data\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame that includes 'person_id' and 'name_org' columns\n",
    "    \"\"\"\n",
    "    orgs_df = pd.DataFrame(people_dict['organizations'])[['id', 'name']]\n",
    "    return (\n",
    "        pd.DataFrame(people_dict['memberships'])\n",
    "        .sort_values('start_date', ascending=False)\n",
    "        .drop_duplicates('person_id')\n",
    "        .merge(orgs_df, left_on='on_behalf_of_id', right_on='id', how='left', suffixes=(\"_\", \"_org\"))\n",
    "    )[['person_id', 'post_id', 'start_date', 'start_reason', 'name_org']] \n",
    "\n",
    "def get_weekly_start_date(end_date: str, weeks:int=1) -> str:\n",
    "    \"\"\"Get the start date for a weekly period ending at the specified end_date\n",
    "    \n",
    "    Args:\n",
    "        end_date: The end date of the period, in the format \"YYYY-MM-DD\"\n",
    "        weeks: The number of weeks to go back\n",
    "\n",
    "    Returns:\n",
    "        The start date of the period, in the format \"YYYY-MM-DD\"\n",
    "    \"\"\"\n",
    "    data_end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    weeks_ago = data_end_date - timedelta(weeks=weeks)\n",
    "    return weeks_ago.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def get_speeches_for_period(\n",
    "    debates_df: pd.DataFrame,\n",
    "    labelstore_df: pd.DataFrame,\n",
    "    start_date: str,\n",
    "    end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get the speeches for a given period\n",
    "    \n",
    "    Args:\n",
    "        debates_df: The DataFrame with debates\n",
    "        labelstore_df: The DataFrame with labels\n",
    "        start_date: The start date of the period, in the format \"YYYY-MM-DD\"\n",
    "        end_date: The end date of the period, in the format \"YYYY-MM-DD\"\n",
    "    \"\"\"\n",
    "    return (\n",
    "            debates_df\n",
    "            .query(\"date >= @start_date and date <= @end_date\")\n",
    "            .merge(labelstore_df[[\"id\", \"mission_labels\", \"topic_labels\"]], left_on=\"speech_id\", right_on=\"id\", how='left')\n",
    "            .drop_duplicates(subset=[\"speakername\", \"speech\"])\n",
    "            .assign(\n",
    "                headings=lambda df: np.where(\n",
    "                    df.minor_heading.notna() & (df.minor_heading != \"\"),  # Check if minor_heading is not empty\n",
    "                    df.major_heading.fillna(\"\") + \": \" + df.minor_heading,  # Combine both\n",
    "                    df.major_heading.fillna(\"\")  # Use only major_heading\n",
    "                )\n",
    "            )\n",
    "            .merge(parties_df[['person_id', 'name_org']], on='person_id', how='left', suffixes=('', '_person'))\n",
    "        )\n",
    "\n",
    "def get_debates_headings(debates_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get the counts of debate speeches by major and minor headings\"\"\"\n",
    "    return (\n",
    "        debates_df\n",
    "        .fillna({\"major_heading\": \"\", \"minor_heading\": \"\"})\n",
    "        .groupby(['date', 'major_heading', 'minor_heading', 'headings'])\n",
    "        .agg(counts=('speech_id', 'count'))\n",
    "        .sort_values('date')\n",
    "        .reset_index() \n",
    "    )\n",
    "\n",
    "def get_debates_major_headings(debates_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Get the counts of debate speeches by major headings\"\"\"\n",
    "    return (\n",
    "        debates_df\n",
    "        .groupby([\"date\", \"major_heading\"])\n",
    "        .agg(counts=(\"speech_id\", \"count\"))\n",
    "        .sort_values(\"date\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_debate_text_and_date(debates_df: pd.DataFrame, debate_title: str) -> Tuple[str, str]:\n",
    "    \"\"\"Get the text of a debate given its title\n",
    "    \n",
    "    Args:\n",
    "        debates_df: The DataFrame with debates\n",
    "        debate_title: The title of the debate\n",
    "    \n",
    "    Returns:\n",
    "        A tuple with the debate text and the dates when the debate took place\n",
    "    \"\"\"\n",
    "    _debate = (\n",
    "        debates_df\n",
    "        .query(\"major_heading == @debate_title\")\n",
    "        .sort_values('speech_id')\n",
    "        .to_dict(orient='records')\n",
    "    )\n",
    "    unique_dates = debates_df.query(\"major_heading == @debate_title\").date.unique()\n",
    "    dates = \", \".join(unique_dates)\n",
    "    debate_text = debate_title + \"\\n-----\\n\"\n",
    "    for speech in _debate:\n",
    "        debate_text += f\"{speech['speakername']} ({speech['name_org']})\" + \"\\n\"\n",
    "        debate_text += speech['speech'] + \"\\n\"\n",
    "        debate_text += \"-----\" + \"\\n\"\n",
    "    return debate_text, dates\n",
    "\n",
    "\n",
    "def relevance_check(df: pd.DataFrame, threshold: int = 10, filter: Literal[None, 'relevant', 'not relevant']=None) -> pd.DataFrame:\n",
    "    \"\"\"Filter speeches by relevance threshold\"\"\"\n",
    "    df = (\n",
    "        df\n",
    "        .assign(relevant = lambda df: df['counts'] >= threshold)\n",
    "    )\n",
    "    if filter == 'relevant':\n",
    "        return df.query(\"relevant\")\n",
    "    elif filter == 'not relevant':\n",
    "        return df.query(\"not relevant\")\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_hits(speech: str, keywords_dict: dict) -> Tuple[List, List, List]:\n",
    "    \"\"\"Get keywords and sentences where they appear in a speech\"\"\"\n",
    "\n",
    "    hits_keywords = []\n",
    "    hits_sentences = []\n",
    "    hits_categories = []\n",
    "    marked_sentences = []\n",
    "    # replace Hon. with Hon\n",
    "    speech = speech.replace(\"Hon.\", \"Hon\").replace(\"hon.\", \"hon\")\n",
    "    sents = keywords.split_sentences([speech], ids=[0])[0]\n",
    "\n",
    "    # Fetch general filtering keywords\n",
    "    keywords_general = None\n",
    "    for cat in keywords_dict:\n",
    "        if 'general terms' in cat:\n",
    "            keywords_general = keywords_dict[cat]\n",
    "            general_cat = cat\n",
    "    if keywords_general is not None:\n",
    "        general_hits = np.array([keywords.find_keyword_hits(kw, sents) for kw in keywords_general]).any(axis=0)\n",
    "    else:\n",
    "        general_hits = [True] * len(sents)\n",
    "        general_cat = \"not specified\"\n",
    "\n",
    "    for cat in keywords_dict:\n",
    "        for kw in keywords_dict[cat]:\n",
    "            hits = keywords.find_keyword_hits(kw, sents)\n",
    "            for i, hit in enumerate(hits):\n",
    "                if (hit and general_hits[i]):\n",
    "                    # print('----')\n",
    "                    # print(kw)\n",
    "                    # print(sents[i])\n",
    "                    hits_keywords.append(kw)\n",
    "                    hits_sentences.append(sents[i])\n",
    "                    hits_categories.append(cat)\n",
    "\n",
    "                    # Add asterisks around the full words containing the matched keyword\n",
    "                    marked_sentence = sents[i]\n",
    "                    for keyword in kw:\n",
    "                        # Regex to find substrings and expand to full words\n",
    "                        pattern = r'\\b(\\S*' + re.escape(keyword) + r'\\S*)\\b'\n",
    "                        marked_sentence = re.sub(pattern, r'*\\1*', marked_sentence)\n",
    "                    \n",
    "                    marked_sentences.append(marked_sentence)   \n",
    "                \n",
    "    # print(f\"Hit for {hits_keywords} in sentences {hits_sentences}\")\n",
    "    # return hits_categories, hits_keywords, marked_sentences\n",
    "    df = (\n",
    "        pd.DataFrame({\n",
    "            'category': hits_categories,\n",
    "            'keyword': hits_keywords,\n",
    "            'sentence': hits_sentences,\n",
    "            'marked_sentence': marked_sentences,\n",
    "        })\n",
    "        .query(\"category != @general_cat\")\n",
    "        .groupby('sentence')\n",
    "        # unique category and keyword for each sentence\n",
    "        .agg(category=('category', list), keyword=('keyword', list), marked_sentence=('marked_sentence', 'first'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024-10-04', '2024-10-11', '2024-10-18', '2024-10-25']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_fridays_last_month(today):\n",
    "    # Convert today's date to a datetime object\n",
    "    today = datetime.strptime(today, \"%Y-%m-%d\")\n",
    "\n",
    "    # Get the first day of the current month\n",
    "    first_day_this_month = today.replace(day=1)\n",
    "\n",
    "    # Get the last day of the previous month\n",
    "    last_day_previous_month = first_day_this_month - timedelta(days=1)\n",
    "\n",
    "    # Get the first day of the previous month\n",
    "    first_day_previous_month = last_day_previous_month.replace(day=1)\n",
    "\n",
    "    # Find all Fridays in the previous month\n",
    "    fridays = []\n",
    "    current_date = first_day_previous_month\n",
    "    while current_date <= last_day_previous_month:\n",
    "        if current_date.weekday() == 4:  # 4 corresponds to Friday\n",
    "            fridays.append(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "    fridays = [date.strftime(\"%Y-%m-%d\") for date in fridays]\n",
    "\n",
    "    return fridays\n",
    "\n",
    "# Get all Fridays of the past month\n",
    "fridays_last_month = get_all_fridays_last_month(\"2024-11-03\")\n",
    "\n",
    "# Display the result\n",
    "fridays_last_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties_df = people_party_memberships(people_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = {}\n",
    "for mission in ['ASF', 'AFS', 'AHL']:\n",
    "    keywords_dict[mission] = keywords.get_keywords(mission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for message_date in fridays_last_month:\n",
    "#     mission = 'AHL'\n",
    "#     # message_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "#     # message_date = \"2024-10-10\"\n",
    "#     # message_date = \"2024-11-10\"\n",
    "#     data_end_date = message_date\n",
    "#     data_start_date = get_weekly_start_date(data_end_date, weeks=1)\n",
    "\n",
    "#     # Get the speeches of the preceding week\n",
    "#     weekly_speeches_df = get_speeches_for_period(\n",
    "#         debates_df=debates_df,\n",
    "#         labelstore_df=labelstore_df,\n",
    "#         start_date=data_start_date,\n",
    "#         end_date=data_end_date\n",
    "#     )\n",
    "#     # Select only debates related to one of the missions\n",
    "#     mission_debates_df = weekly_speeches_df.query(\"mission_labels == @mission\")\n",
    "#     # Get the major headings\n",
    "#     mission_debates_major_headings_df = get_debates_major_headings(mission_debates_df)\n",
    "#     # Filter the debates by relevance (simple threshold)\n",
    "#     debates_to_summarise_df = relevance_check(mission_debates_major_headings_df, filter='relevant')\n",
    "#     # Get unique debate titles\n",
    "#     debate_titles = debates_to_summarise_df.major_heading.unique()\n",
    "\n",
    "#     print(message_date)\n",
    "#     print(debate_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_more_robust_keywords(mission_debates_df, keywords_dict):\n",
    "    _df = mission_debates_df.copy()\n",
    "    _sentences = []\n",
    "    _categories = []\n",
    "    _keywords = []\n",
    "    for _, row in mission_debates_df.iterrows():\n",
    "        df = get_keyword_hits(row['speech'], keywords_dict)\n",
    "        _sentences.append(df['sentence'].to_list())\n",
    "        _categories.append(df['category'].to_list())\n",
    "        _keywords.append(df['keyword'].to_list())\n",
    "    _df['sentence'] = _sentences\n",
    "    _df['category'] = _categories\n",
    "    _df['keyword'] = _keywords\n",
    "    _df = _df.assign(n_sentences = lambda df: df['sentence'].apply(len))\n",
    "    return _df.query(\"n_sentences > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>major_heading</th>\n",
       "      <th>counts</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-12-04</td>\n",
       "      <td>End of Radio Teleswitch Service:  Rural Areas</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                  major_heading  counts  relevant\n",
       "4  2024-12-04  End of Radio Teleswitch Service:  Rural Areas       8      True"
      ]
     },
     "execution_count": 829,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mission = 'ASF'\n",
    "# mission = 'AFS'\n",
    "# mission = 'AHL'\n",
    "message_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "# message_date = \"2024-11-08\"\n",
    "# message_date = \"2024-11-22\"\n",
    "# message_date = \"2024-12-09\"\n",
    "# message_date = \"2024-10-10\"\n",
    "# message_date = \"2021-05-28\"\n",
    "data_end_date = message_date\n",
    "data_start_date = get_weekly_start_date(data_end_date, weeks=1)\n",
    "\n",
    "# Get the speeches of the preceding week\n",
    "weekly_speeches_df = get_speeches_for_period(\n",
    "    debates_df=debates_df,\n",
    "    labelstore_df=labelstore_df,\n",
    "    start_date=data_start_date,\n",
    "    end_date=data_end_date\n",
    ")\n",
    "# Select only debates related to one of the missions\n",
    "# mission_debates_df = weekly_speeches_df.query(\"mission_labels == @mission\")\n",
    "# mission_debates_df = check_more_robust_keywords(mission_debates_df, keywords_dict[mission])\n",
    "mission_debates_df = check_more_robust_keywords(weekly_speeches_df, keywords_dict[mission])\n",
    "# Get the major headings\n",
    "mission_debates_major_headings_df = get_debates_major_headings(mission_debates_df)\n",
    "# Filter the debates by relevance (simple threshold)\n",
    "debates_to_summarise_df = relevance_check(mission_debates_major_headings_df, threshold = 5, filter='relevant')\n",
    "# Get unique debate titles\n",
    "debate_titles = debates_to_summarise_df.major_heading.unique()\n",
    "debates_to_summarise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(synthesis_utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_dicts = []\n",
    "not_relevant_titles = []\n",
    "for debate_title in debate_titles:\n",
    "    # Prepare the debate text\n",
    "    debate_text, debate_date = get_debate_text_and_date(weekly_speeches_df, debate_title)\n",
    "    debate_instance = synthesis_utils.Debate(heading=debate_title, content=debate_text)  \n",
    "    # Check relevance\n",
    "    relevant = synthesis_utils.classify_relevance(debate_text, mission).relevant \n",
    "    if relevant:\n",
    "        # Call LLM\n",
    "        logging.info(f\"Summarising debate: {debate_title}\")\n",
    "        result = synthesis_utils.summarise_debate_with_structure(debate_instance)\n",
    "        # Get debate id\n",
    "        debate_id = weekly_speeches_df.query(\"major_heading == @debate_title\").speech_id.iloc[0]\n",
    "        debate_id = debate_id.split(\"/\")[-1]\n",
    "        debate_id = f'{\".\".join(debate_id.split(\".\")[0:-1])}.0'        \n",
    "        debate_url = f\"https://www.theyworkforyou.com/debates/?id={debate_id}\"\n",
    "        # Prepare the debate dictionary\n",
    "        debate_dict = result.model_dump()\n",
    "        debate_dict['title'] = debate_title\n",
    "        debate_dict['date'] = debate_date\n",
    "        debate_dict['url'] = debate_url\n",
    "        debate_dicts.append(debate_dict)\n",
    "    else:\n",
    "        not_relevant_titles.append(debate_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['End of Radio Teleswitch Service:  Rural Areas']"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_relevant_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speech_context(debates_df: pd.DataFrame, speech_dict: dict) -> str: \n",
    "    # Locate the speech index\n",
    "    speech_index = debates_df[debates_df['speech_id'] == speech_dict['speech_id']].index[0]\n",
    "    \n",
    "    # Retrieve previous, current, and next speech data\n",
    "    # Quick hack to avoid index out of bounds\n",
    "    try:\n",
    "        prev_speech = debates_df.iloc[speech_index - 1]\n",
    "    except IndexError:\n",
    "        prev_speech = debates_df.iloc[speech_index]\n",
    "    current_speech = debates_df.iloc[speech_index]\n",
    "    try:\n",
    "        next_speech = debates_df.iloc[speech_index + 1]\n",
    "    except IndexError:\n",
    "        next_speech = debates_df.iloc[speech_index]    \n",
    "    \n",
    "    \n",
    "    # Format and return the speech context\n",
    "    return (\n",
    "        f\"# PREVIOUS SPEECH\\n\"\n",
    "        f\"Speaker: {prev_speech.speakername} ({prev_speech.name_org})\\n\"\n",
    "        f\"Full speech: {prev_speech.speech}\\n\"\n",
    "        f\"# SPEECH WITH KEYWORDS\\n\"\n",
    "        f\"Keywords: {speech_dict['keyword']}\\n\"\n",
    "        f\"Sentences with keywords: {speech_dict['sentence']}\\n\"\n",
    "        f\"Speaker: {current_speech.speakername} ({current_speech.name_org})\\n\"\n",
    "        f\"Full speech: {current_speech.speech}\\n\"\n",
    "        f\"# NEXT SPEECH\\n\"\n",
    "        f\"Speaker: {next_speech.speakername} ({next_speech.name_org})\\n\"\n",
    "        f\"Full speech: {next_speech.speech}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>major_heading</th>\n",
       "      <th>counts</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>Grenfell Tower Inquiry</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-12-02</td>\n",
       "      <td>Housing, Communities and Local Government</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-12-03</td>\n",
       "      <td>National Insurance Contributions (Secondary Cl...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-12-03</td>\n",
       "      <td>UK Supply Chains:  Uyghur Forced Labour</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-12-04</td>\n",
       "      <td>Farming and Inheritance Tax</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-12-04</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>Business of the House</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>Cabinet Office</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-12-05</td>\n",
       "      <td>Improving Public Transport</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-12-06</td>\n",
       "      <td>Spray Foam Insulation: Property Value</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                      major_heading  counts  \\\n",
       "0   2024-12-02                             Grenfell Tower Inquiry       3   \n",
       "1   2024-12-02          Housing, Communities and Local Government       3   \n",
       "2   2024-12-03  National Insurance Contributions (Secondary Cl...       3   \n",
       "3   2024-12-03            UK Supply Chains:  Uyghur Forced Labour       2   \n",
       "5   2024-12-04                        Farming and Inheritance Tax       4   \n",
       "6   2024-12-04                                           Scotland       2   \n",
       "7   2024-12-05                              Business of the House       2   \n",
       "8   2024-12-05                                     Cabinet Office       3   \n",
       "9   2024-12-05                         Improving Public Transport       1   \n",
       "10  2024-12-06              Spray Foam Insulation: Property Value       2   \n",
       "\n",
       "    relevant  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "3      False  \n",
       "5      False  \n",
       "6      False  \n",
       "7      False  \n",
       "8      False  \n",
       "9      False  \n",
       "10     False  "
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debates with few mentions\n",
    "debates_not_relevant_df = relevance_check(mission_debates_major_headings_df, threshold=5, filter='not relevant')\n",
    "debates_not_relevant_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_to_report = []\n",
    "major_headings_not_relevant = mission_debates_major_headings_df.major_heading.unique()\n",
    "for major_heading in major_headings_not_relevant:\n",
    "    speech_dicts = mission_debates_df.query(\"major_heading == @major_heading\").to_dict(orient='records')\n",
    "    n_speeches = len(speech_dicts)\n",
    "    for speech_dict in speech_dicts:\n",
    "        text = get_speech_context(weekly_speeches_df, speech_dict)\n",
    "        _speech_dict = speech_dict.copy()\n",
    "        _speech_dict['text'] = text\n",
    "        relevant = synthesis_utils.classify_relevance(text, mission).relevant\n",
    "        if relevant:\n",
    "            summary = synthesis_utils.summarise_quote(text).summary\n",
    "            _speech_dict['relevant'] = relevant\n",
    "            _speech_dict['summary'] = summary\n",
    "            speeches_to_report.append(_speech_dict)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for major_heading in major_headings_not_relevant:\n",
    "#     speech_dicts = mission_debates_df.query(\"major_heading == @major_heading\").to_dict(orient='records')\n",
    "#     for speech_dict in speech_dicts:\n",
    "#         print(speech_dict['speakername'])\n",
    "#         print(speech_dict['speech'])\n",
    "#         print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(speeches_to_report)\n",
    "if len(df) > 0:\n",
    "    unique_headings = df.major_heading.unique()\n",
    "    quote_dicts = []\n",
    "    for heading in unique_headings:\n",
    "        df_report = df.query(\"major_heading == @heading\")\n",
    "        speech_dicts = df_report.to_dict(orient='records')\n",
    "        quotes = []\n",
    "        for speech_dict in speech_dicts:\n",
    "            quote_id = speech_dict['speech_id'].split(\"/\")[-1]\n",
    "            quote_url = f\"https://www.theyworkforyou.com/debates/?id={quote_id}\"\n",
    "            quotes.append({\"summary\": speech_dict['summary'], \"url\": quote_url})\n",
    "        quote_dict = {\n",
    "            \"heading\": heading,\n",
    "            \"date\": \"; \".join(df_report.date.unique()),\n",
    "            \"quotes\": quotes,\n",
    "        }\n",
    "        quote_dicts.append(quote_dict)\n",
    "else:\n",
    "    quote_dicts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_debate_block(quote_dict: Dict) -> List[Dict]:\n",
    "    \"\"\"Construct a block with a quote\n",
    "    \n",
    "    Args:\n",
    "        quote (Dict): Dictionary with keys \"name\", \"party\", \"category\", \"debate\", and \"text\".\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        \"type\": \"section\",\n",
    "        \"text\": {\n",
    "            \"type\": \"mrkdwn\",\n",
    "            \"text\": f\"*{quote_dict['heading']}*: Highlights on {quote_dict['date']}.\"\n",
    "        }\n",
    "    }\n",
    "    quote_blocks = []\n",
    "    for quote in quote_dict['quotes']:\n",
    "        _quote = {\n",
    "            \"type\": \"section\",\n",
    "            \"text\":{\n",
    "                \"type\": \"mrkdwn\",\n",
    "                \"text\": f\"{quote['summary']} (<{quote['url']}|source>)\"\n",
    "            }\n",
    "        }\n",
    "        quote_blocks.append(_quote)\n",
    "    return [summary] + quote_blocks + [divider()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quotes = []\n",
    "# for debate_title in debates_not_relevant_df.major_heading.to_list():\n",
    "#     df = mission_debates_df.query(\"major_heading == @debate_title\")\n",
    "#     for _, row in df.iterrows():\n",
    "#         cat_hits, kw_hits, sentences = get_keyword_hits(row['speech'], keywords_dict[mission])\n",
    "#         if len(kw_hits) == 0:\n",
    "#             continue\n",
    "#         text = \" .. \".join(sentences)\n",
    "\n",
    "#         quote = {\n",
    "#             \"name\": row['speakername'],\n",
    "#             \"party\": row['name_org'],\n",
    "#             \"category\": row['topic_labels'],\n",
    "#             \"debate\": debate_title,\n",
    "#             \"text\": text,\n",
    "#             \"keywords\": kw_hits\n",
    "#         }\n",
    "#         quotes.append(quote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_blocks = [debate_summary(debate) for debate in debate_dicts]\n",
    "debate_blocks = [item for sublist in debate_blocks for item in sublist]\n",
    "\n",
    "# quote_blocks = [quote_block(quote) for quote in quotes]\n",
    "quote_blocks = [quote_debate_block(quote_dict) for quote_dict in quote_dicts]\n",
    "# unnest\n",
    "quote_blocks = [item for sublist in quote_blocks for item in sublist]\n",
    "\n",
    "blocks = message_header(message_date, data_start_date, data_end_date)\n",
    "blocks += [mission_header(mission)]\n",
    "blocks += debate_blocks\n",
    "blocks += quote_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'header',\n",
       "  'text': {'type': 'plain_text', 'text': 'Policy update 2024-12-09'}},\n",
       " {'type': 'context',\n",
       "  'elements': [{'type': 'mrkdwn',\n",
       "    'text': 'House of Commons debates (2024-12-02 - 2024-12-09)'}]},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn', 'text': ':potted_plant: *A Sustainable Future*'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Cabinet Office*: Highlights on 2024-12-05.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Onn (Lab) asked about incorporating *green skills* to create new jobs in North East Lincolnshire, Grimsby, and Cleethorpes. (<https://www.theyworkforyou.com/debates/?id=2024-12-05b.440.5|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Spray Foam Insulation: Property Value*: Highlights on 2024-12-06.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Gordon (Lib Dem) highlighted issues with *insulation* and *energy efficiency* from the green homes grant scheme, stressing the need for government action to support affected homeowners. (<https://www.theyworkforyou.com/debates/?id=2024-12-06a.641.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Fahnbulleh (Lab/Co-op) said the Government is committed to high standards in energy efficiency installations and consumer protection, acknowledging difficulties faced by homeowners with spray foam insulation in obtaining finance. (<https://www.theyworkforyou.com/debates/?id=2024-12-06a.645.0|source>)'}},\n",
       " {'type': 'divider'}]"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pretty print the blocks\n",
    "# import json\n",
    "\n",
    "# print(json.dumps(blocks, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(synthesis_utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the message with blocks\n",
    "response = slack_webhook.send(blocks=blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All missions combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 18:27:00,656 - root - INFO - Summarising debate: SEND Provision: Autism and ADHD\n"
     ]
    }
   ],
   "source": [
    "mission_blocks = []\n",
    "for mission in ['ASF', 'AFS', 'AHL']:\n",
    "    message_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    data_end_date = message_date\n",
    "    data_start_date = get_weekly_start_date(data_end_date, weeks=1)\n",
    "\n",
    "    # Get the speeches of the preceding week\n",
    "    weekly_speeches_df = get_speeches_for_period(\n",
    "        debates_df=debates_df,\n",
    "        labelstore_df=labelstore_df,\n",
    "        start_date=data_start_date,\n",
    "        end_date=data_end_date\n",
    "    )\n",
    "    # Select only debates related to one of the missions\n",
    "    # mission_debates_df = weekly_speeches_df.query(\"mission_labels == @mission\")\n",
    "    # mission_debates_df = check_more_robust_keywords(mission_debates_df, keywords_dict[mission])\n",
    "    mission_debates_df = check_more_robust_keywords(weekly_speeches_df, keywords_dict[mission])\n",
    "    # Get the major headings\n",
    "    mission_debates_major_headings_df = get_debates_major_headings(mission_debates_df)\n",
    "    # Filter the debates by relevance (simple threshold)\n",
    "    debates_to_summarise_df = relevance_check(mission_debates_major_headings_df, threshold = 5, filter='relevant')\n",
    "    # Get unique debate titles\n",
    "    debate_titles = debates_to_summarise_df.major_heading.unique()\n",
    "    debates_to_summarise_df\n",
    "\n",
    "\n",
    "    debate_dicts = []\n",
    "    not_relevant_titles = []\n",
    "    for debate_title in debate_titles:\n",
    "        # Prepare the debate text\n",
    "        debate_text, debate_date = get_debate_text_and_date(weekly_speeches_df, debate_title)\n",
    "        debate_instance = synthesis_utils.Debate(heading=debate_title, content=debate_text)  \n",
    "        # Check relevance\n",
    "        relevant = synthesis_utils.classify_relevance(debate_text, mission).relevant \n",
    "        if relevant:\n",
    "            # Call LLM\n",
    "            logging.info(f\"Summarising debate: {debate_title}\")\n",
    "            result = synthesis_utils.summarise_debate_with_structure(debate_instance)\n",
    "            # Get debate id\n",
    "            debate_id = weekly_speeches_df.query(\"major_heading == @debate_title\").speech_id.iloc[0]\n",
    "            debate_id = debate_id.split(\"/\")[-1]\n",
    "            debate_id = f'{\".\".join(debate_id.split(\".\")[0:-1])}.0'        \n",
    "            debate_url = f\"https://www.theyworkforyou.com/debates/?id={debate_id}\"\n",
    "            # Prepare the debate dictionary\n",
    "            debate_dict = result.model_dump()\n",
    "            debate_dict['title'] = debate_title\n",
    "            debate_dict['date'] = debate_date\n",
    "            debate_dict['url'] = debate_url\n",
    "            debate_dicts.append(debate_dict)\n",
    "        else:\n",
    "            not_relevant_titles.append(debate_title)\n",
    "\n",
    "    # Debates with few mentions\n",
    "    debates_not_relevant_df = relevance_check(mission_debates_major_headings_df, threshold=5, filter='not relevant')\n",
    "        \n",
    "    speeches_to_report = []\n",
    "    major_headings_not_relevant = mission_debates_major_headings_df.major_heading.unique()\n",
    "    for major_heading in major_headings_not_relevant:\n",
    "        speech_dicts = mission_debates_df.query(\"major_heading == @major_heading\").to_dict(orient='records')\n",
    "        n_speeches = len(speech_dicts)\n",
    "        for speech_dict in speech_dicts:\n",
    "            text = get_speech_context(weekly_speeches_df, speech_dict)\n",
    "            _speech_dict = speech_dict.copy()\n",
    "            _speech_dict['text'] = text\n",
    "            relevant = synthesis_utils.classify_relevance(text, mission).relevant\n",
    "            if relevant:\n",
    "                summary = synthesis_utils.summarise_quote(text).summary\n",
    "                _speech_dict['relevant'] = relevant\n",
    "                _speech_dict['summary'] = summary\n",
    "                speeches_to_report.append(_speech_dict)  \n",
    "\n",
    "    df = pd.DataFrame(speeches_to_report)\n",
    "    if len(df) > 0:\n",
    "        unique_headings = df.major_heading.unique()\n",
    "        quote_dicts = []\n",
    "        for heading in unique_headings:\n",
    "            df_report = df.query(\"major_heading == @heading\")\n",
    "            speech_dicts = df_report.to_dict(orient='records')\n",
    "            quotes = []\n",
    "            for speech_dict in speech_dicts:\n",
    "                quote_id = speech_dict['speech_id'].split(\"/\")[-1]\n",
    "                quote_url = f\"https://www.theyworkforyou.com/debates/?id={quote_id}\"\n",
    "                quotes.append({\"summary\": speech_dict['summary'], \"url\": quote_url})\n",
    "            quote_dict = {\n",
    "                \"heading\": heading,\n",
    "                \"date\": \"; \".join(df_report.date.unique()),\n",
    "                \"quotes\": quotes,\n",
    "            }\n",
    "            quote_dicts.append(quote_dict)\n",
    "    else:\n",
    "        quote_dicts = []                          \n",
    "        \n",
    "    debate_blocks = [debate_summary(debate) for debate in debate_dicts]\n",
    "    debate_blocks = [item for sublist in debate_blocks for item in sublist]\n",
    "\n",
    "    # quote_blocks = [quote_block(quote) for quote in quotes]\n",
    "    quote_blocks = [quote_debate_block(quote_dict) for quote_dict in quote_dicts]\n",
    "    # unnest\n",
    "    quote_blocks = [item for sublist in quote_blocks for item in sublist]\n",
    "\n",
    "    blocks = message_header(message_date, data_start_date, data_end_date)\n",
    "    blocks += [mission_header(mission)]\n",
    "    blocks += debate_blocks\n",
    "    blocks += quote_blocks\n",
    "\n",
    "    mission_blocks.append(blocks)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mission_blocks = mission_blocks[0] + mission_blocks[2][2:] + mission_blocks[1][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'header',\n",
       "  'text': {'type': 'plain_text', 'text': 'Policy update 2024-12-16'}},\n",
       " {'type': 'context',\n",
       "  'elements': [{'type': 'mrkdwn',\n",
       "    'text': 'House of Commons debates (2024-12-09 - 2024-12-16)'}]},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn', 'text': ':potted_plant: *A Sustainable Future*'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Finance Bill*: Highlights on 2024-12-10.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Yang (Lab) said that the energy profits levy will raise £2.3 billion for funding *Great British Energy*, which will innovate in *green technologies* across the UK. (<https://www.theyworkforyou.com/debates/?id=2024-12-10c.843.1|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn', 'text': ':hatched_chick: *A Fairer Start*'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1139.0|*SEND Provision: Autism and ADHD*> (2024-12-12)\\nTo address the crisis in special educational needs and disability support for children with autism and ADHD.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Positives*\\n• Pippa Heylings highlighted the inspiring work of staff at Bassingbourn primary school, creating safe spaces for students (Lib Dem).\\n• Mark Sewards noted that inclusive education efforts continue despite previous government stresses (Lab).\\n• Catherine McKinnell emphasised the importance of good education in breaking down barriers for vulnerable children (Lab).'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Negatives*\\n• Pippa Heylings described a broken system where children must fail before receiving necessary support (Lib Dem).\\n• Sarah Russell pointed out long waits for diagnosis, especially for deprived children (Lab).\\n• Pippa Heylings mentioned a postcode lottery in SEND support, with stark regional disparities in waiting times (Lib Dem).'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"*Next Steps*\\n• Pippa Heylings called for a national SEND body to end the postcode lottery (Lib Dem).\\n• Catherine McKinnell stressed the need for system reform to regain families' confidence (Lab).\\n• Pippa Heylings urged for increased funding for diagnostic services and educational psychologists (Lib Dem).\"}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Education*: Highlights on 2024-12-09.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Phillipson (Lab) said that *breakfast clubs* improve *childcare* for *parents* and enhance children's readiness to learn. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.648.4|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Voaden (Lib Dem) said that statutory breakfast programmes may favour wealthier families and asked if expanding school lunches and auto-enrolment for free meals would help tackle food insecurity. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.649.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Turmaine (Lab) expressed concerns from *parents* about education and asked for details on the benefits of breakfast clubs. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.652.1|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Morgan (Lab) said the Government is committed to providing free breakfast clubs in every primary school to ensure children are ready to learn, and announced a tripling of investment in breakfast clubs for improved behaviour, attendance, and more childcare choices for parents. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.652.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Burton-Sampson (Lab) asked if breakfast clubs could help reduce child poverty and support parents' return to part-time *employment*. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.652.3|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Morgan (Lab) said breakfast clubs support parents and carers by providing access to breakfast and assisting with childcare costs. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.652.4|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Lopez (Con) said nurseries are facing challenges due to national insurance increases and questioned the Government's clarity on breakfast clubs. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.652.5|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Phillipson (Lab) said that the government aims to transform early years education by expanding *nurseries* and rolling out *childcare* to ensure children are ready for school. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.663.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Wilson (Lib Dem) highlighted that deprived areas have a third fewer *childcare* places and urged the Government to consider tripling the early years pupil premium to improve *read*iness among children. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.665.0|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Phillipson (Lab) said that rolling out *nursery* space within primary schools is crucial for creating the required places. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.665.1|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Morgan (Lab) said that the plan for change will improve support through *pregnancy* and early childhood. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.665.7|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Hayes (Lab) asked how the Secretary of State will ensure that *childcare* is central to the Government’s child poverty strategy, highlighting issues in accessing *childcare* in disadvantaged areas. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.666.7|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Hayes (Lab) asked about steps to improve *childcare* apprenticeships after visiting *nursery* facilities. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.668.3|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Smith (Lab) welcomed the Government’s ambition for 40,000 extra children to be school ready but highlighted the lack of extra classrooms for *nursery* provision in her constituency. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.670.3|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Morgan (Lab) said the Government are committed to rolling out school-based *nurseries*. (<https://www.theyworkforyou.com/debates/?id=2024-12-09b.670.4|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Finance Bill*: Highlights on 2024-12-11.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Ghani (Con) said the review must assess the impact of VAT provisions on children with *special needs* without an Education Health and Care Plan in relation to new clauses on private school fees. (<https://www.theyworkforyou.com/debates/?id=2024-12-11b.936.6|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Wilson (Lib Dem) said that the SEND system is failing and that it is unfair to penalise parents who have made sacrifices for their children's special needs. (<https://www.theyworkforyou.com/debates/?id=2024-12-11b.950.1|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Sandher (Lab) said the *childcare* expansion will create more jobs and increase women's workforce participation. (<https://www.theyworkforyou.com/debates/?id=2024-12-11b.953.4|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Murray (Lab) clarified the treatment of *nurseries* in the Bill, stating that fully stand-alone *nurseries* are not covered. (<https://www.theyworkforyou.com/debates/?id=2024-12-11b.965.3|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Business and Trade*: Highlights on 2024-12-12.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Madders (Lab) recognised the vital role of kinship carers in balancing *employment* and care, announcing £40 million for support and a review of *parental leave*. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1019.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Wilson (Lib Dem) highlighted that four in 10 kinship carers must give up *work* to care for children whose *parents* cannot, and urged the Minister to address this through the Employment Rights Bill. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1019.3|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Madders (Lab) said they are reviewing the *parental leave* system and will observe the trial outcomes. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1019.4|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*Business of the House*: Highlights on 2024-12-12.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Young (Lib Dem) asked for a debate on ensuring fair arrangements for children with *disabilities* and special educational needs. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1043.0|source>)'}},\n",
       " {'type': 'divider'},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': '*SEND Provision: Autism and ADHD*: Highlights on 2024-12-12.'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Heylings (Lib Dem) said that parents face challenges with mainstream schools for children with *autism* and *ADHD*, highlighting a 270% increase in autism cases and a postcode lottery in accessing support. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1140.3|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Woodcock (Lab) asked if the issues in education are due to the lack of recognition for support staff and their role in children's education. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1141.0|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'Young (Lib Dem) said it is an outrage that some *parents* face prosecution when their children miss school due to lack of support. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1143.0|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': \"Heylings (Lib Dem) said that reduced timetables in mainstream schools hinder parents' ability to work, highlighting the need for a national SEND body and better support for children. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1143.1|source>)\"}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'McKinnell (Lab) said that despite rising funding for children with *disabilities*, the system is failing to deliver the necessary outcomes, highlighting the need for reform. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1143.2|source>)'}},\n",
       " {'type': 'section',\n",
       "  'text': {'type': 'mrkdwn',\n",
       "   'text': 'McKinnell (Lab) said it is vital to work with *parents*, schools, and health sectors to support children with *autism* and *ADHD*, highlighting the increase in identified cases and the need for effective support. (<https://www.theyworkforyou.com/debates/?id=2024-12-12a.1145.1|source>)'}},\n",
       " {'type': 'divider'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mission_blocks[0] + mission_blocks[1][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mission_blocks[0] + mission_blocks[1][2:] # + mission_blocks[2][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the message with blocks\n",
    "response = slack_webhook.send(blocks=_mission_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery-mission-radar-prototyping-ejbE0IFh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
