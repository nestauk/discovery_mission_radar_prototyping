{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import discovery_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import PROJECT_DIR\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery_utils.utils.llm.batch_check import (\n",
    "    LLMProcessor,\n",
    "    generate_system_message\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crunchbase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['yes', 'no', 'maybe']\n",
    "review_labels = {\n",
    "    'y': 'yes',\n",
    "    'Yes - CR': 'yes',\n",
    "    'Yes - CR ': 'yes',\n",
    "    'y-CR': 'yes',\n",
    "    'Maybe - CR': 'maybe',\n",
    "    'maybe': 'maybe',\n",
    "    'Maybe': 'maybe',\n",
    "    'n': 'no',\n",
    "    'no': 'no',\n",
    "    'No - CR': 'no',\n",
    "    'No': 'no'\n",
    "}\n",
    "\n",
    "reviewed_data_df = (\n",
    "    pd.read_csv(PROJECT_DIR / \"data/2024_12_MS/Cybersecurity - Mission studio 2012-12-16 - crunchbase.csv\")\n",
    "    .rename(columns={\"RELEVANT?\": \"relevant\"})\n",
    "    .assign(relevant=lambda x: x[\"relevant\"].map(review_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_check_df = reviewed_data_df.query(\"relevant in @labels\").copy()\n",
    "text_data = dict(zip(data_to_check_df[\"id\"].tolist(), data_to_check_df[\"short_description\"].tolist()))\n",
    "len(data_to_check_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = PROJECT_DIR / \"data/2024_12_MS/llm_check_v3.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "system_message = generate_system_message(\"config.yaml\")\n",
    "\n",
    "processor = LLMProcessor(\n",
    "    output_path=output_file,\n",
    "    system_message=system_message,\n",
    "    session_name=\"testing\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(data_to_check_df, output_file) -> pd.DataFrame:\n",
    "    df = (\n",
    "        pd.read_json(output_file, lines=True)\n",
    "        .merge(data_to_check_df, on='id', how='left')\n",
    "    )\n",
    "    # make a confusion matrix between columns is_relevant and relevant\n",
    "    confusion_matrix = pd.crosstab(df[\"is_relevant\"], df[\"relevant\"])\n",
    "    # make a confusion matrix with percentages\n",
    "    confusion_matrix_p = confusion_matrix.div(confusion_matrix.sum(axis=0), axis=1)\n",
    "    return confusion_matrix, confusion_matrix_p, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, confusion_matrix_p, df = get_confusion_matrix(data_to_check_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"relevant != 'maybe'\").assign(agree=lambda x: x[\"is_relevant\"] == x[\"relevant\"])[\"agree\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"is_relevant == 'no' and relevant == 'yes'\")[\"short_description\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check with gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = PROJECT_DIR / \"data/2024_12_MS/llm_check_gpt4o.jsonl\"\n",
    "gpt_model = \"gpt-4o\"\n",
    "\n",
    "processor = LLMProcessor(\n",
    "    output_path=output_file,\n",
    "    system_message=system_message,\n",
    "    session_name=\"testing\",\n",
    "    model_name=gpt_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt4o = (\n",
    "    pd.read_json(output_file, lines=True)\n",
    "    .merge(data_to_check_df, on='id', how='left')\n",
    ")\n",
    "# make a confusion matrix between columns is_relevant and relevant\n",
    "confusion_matrix = pd.crosstab(df_gpt4o[\"is_relevant\"], df_gpt4o[\"relevant\"])\n",
    "# make a confusion matrix with percentages\n",
    "confusion_matrix_p = confusion_matrix.div(confusion_matrix.sum(axis=0), axis=1)\n",
    "confusion_matrix_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt4o.query(\"relevant != 'maybe'\").assign(agree=lambda x: x[\"is_relevant\"] == x[\"relevant\"])[\"agree\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.merge(df_gpt4o[['id', 'is_relevant']], on='id', suffixes=('', '_gpt4o'))\n",
    "pd.crosstab(df_merged[\"is_relevant\"], df_merged[\"is_relevant_gpt4o\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.query(\"is_relevant == 'no' and is_relevant_gpt4o == 'yes'\")[\"short_description\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UKRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['yes', 'no', 'maybe']\n",
    "review_labels = {\n",
    "    'y': 'yes',\n",
    "    'yes': 'yes',\n",
    "    'Maybe?': 'maybe',\n",
    "    'n': 'no',\n",
    "}\n",
    "\n",
    "reviewed_data_df = (\n",
    "    pd.read_csv(PROJECT_DIR / \"data/2024_12_MS/Cybersecurity - Mission studio 2012-12-16 - ukri.csv\")\n",
    "    .rename(columns={\"RELEVANT?\": \"relevant\"})\n",
    "    .assign(relevant=lambda x: x[\"relevant\"].map(review_labels))\n",
    ")\n",
    "\n",
    "data_to_check_df = reviewed_data_df.query(\"relevant in @labels\").copy()\n",
    "text_data = dict(zip(data_to_check_df[\"id\"].tolist(), data_to_check_df[\"abstractText\"].tolist()))\n",
    "len(data_to_check_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = PROJECT_DIR / \"data/2024_12_MS/llm_check_gtr_gpt-4o-mini.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = generate_system_message(\"config.yaml\")\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "fields = [\n",
    "    {\"name\": \"is_relevant\", \"type\": \"str\", \"description\": \"A one-word answer: 'yes' or 'no'.\"},\n",
    "    {\"name\": \"relevance_score\", \"type\": \"float\", \"description\": \"Score between 0 to 1 indicating how relevant it is: 0 = not relevant at all; 0.5 = maybe relevant; 1.0 = highly relevant.\"}\n",
    "]\n",
    "processor = LLMProcessor(\n",
    "    output_path=output_file,\n",
    "    system_message=system_message,\n",
    "    session_name=\"testing\",\n",
    "    model_name=gpt_model,\n",
    "    output_fields=fields\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, confusion_matrix_p, df = get_confusion_matrix(data_to_check_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"relevant != 'maybe'\").assign(agree=lambda x: x[\"is_relevant\"] == x[\"relevant\"])[\"agree\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"is_relevant == 'no' and relevant == 'yes'\")[\"title\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "df.query(\"is_relevant == 'no' and relevant == 'no'\")[\"relevance_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "df.query(\"is_relevant == 'no' and relevant == 'yes'\")[\"relevance_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "df.query(\"is_relevant == 'yes' and relevant == 'no'\")[\"relevance_score\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "df.query(\"is_relevant == 'yes' and relevant == 'yes'\")[\"relevance_score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking with gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = PROJECT_DIR / \"data/2024_12_MS/llm_check_gtr_gpt-4o.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = generate_system_message(\"config.yaml\")\n",
    "gpt_model = \"gpt-4o\"\n",
    "fields = [\n",
    "    {\"name\": \"is_relevant\", \"type\": \"str\", \"description\": \"A one-word answer: 'yes' or 'no'.\"},\n",
    "    {\"name\": \"relevance_score\", \"type\": \"float\", \"description\": \"Score between 0 to 1 indicating how relevant it is: 0 = not relevant at all; 0.5 = maybe relevant; 1.0 = highly relevant.\"}\n",
    "]\n",
    "processor = LLMProcessor(\n",
    "    output_path=output_file,\n",
    "    system_message=system_message,\n",
    "    session_name=\"testing\",\n",
    "    model_name=gpt_model,\n",
    "    output_fields=fields\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix, confusion_matrix_p, df = get_confusion_matrix(data_to_check_df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"relevant != 'maybe'\").assign(agree=lambda x: x[\"is_relevant\"] == x[\"relevant\"])[\"agree\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"is_relevant == 'yes' and relevant == 'no'\")[\"title\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stats\n",
    "df.query(\"is_relevant == 'yes' and relevant == 'no'\")[\"relevance_score\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discovery-mission-radar-prototyping-ejbE0IFh-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
